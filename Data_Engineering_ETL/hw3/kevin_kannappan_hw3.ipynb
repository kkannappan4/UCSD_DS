{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "Kevin Kannappan\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Please find a file called apple.tsv in the Files section. It contains 1500 records which are news articles. The first column is a text column containing the actual article. \n",
    "\n",
    "Your task is to use the article text compute a suitable topic model using the Latent Dirichlet Modeling from the gensim package. You must  use the topic coherence metric to determine a suitable number of topics. Your expected output are:\n",
    "\n",
    "a) a list of topics including the top 10 terms in each topic \n",
    "\n",
    "b) the top 10 documents related to each topic along with their topic proportions\n",
    "\n",
    "c) the coherence measure for each run of the model as you determine the suitable number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "news_articles = pd.read_csv('./apple.tsv', header=None,sep='\\t')\n",
    "print(news_articles.shape)\n",
    "news_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning:\n",
    "news_articles.columns = ['article_text', 'article_id','publish_date','article_title','article_url','extra_1','author','tags','base_url','extra_2','extra_3']\n",
    "news_articles.drop(['extra_1', 'extra_2','extra_3'], axis=1,inplace=True)\n",
    "news_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text manipulation\n",
    "# Convert to list\n",
    "news_text = news_articles.article_text.values.tolist()\n",
    "\n",
    "# Remove non-needed characters:\n",
    "news_text = [re.sub(\"\\n\", \" \", article) for article in news_text]\n",
    "news_text = [re.sub(\"\\'\", \"\", article) for article in news_text]\n",
    "news_text = [re.sub('\\s+', ' ', article) for article in news_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    return [[i for i in gensim.utils.simple_preprocess(str(article),deacc=True) if i not in stopwords.words('english')] for article in text]\n",
    "\n",
    "# Unfortunately, this took a bit to run:\n",
    "article_clean_text = clean_text(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_clean_text)==len(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams:\n",
    "# Use original positioning\n",
    "bigram = gensim.models.Phrases(news_text, min_count=5, threshold=100)\n",
    "bigram_phrase = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "bigrams_text = [bigram_phrase[article] for article in article_clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_text==article_clean_text\n",
    "# No notable bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish necessary components for LDA:\n",
    "# Create dict and corpus\n",
    "id2word = gensim.corpora.Dictionary(bigrams_text)\n",
    "corpus = [id2word.doc2bow(text) for text in bigrams_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View corpus\n",
    "print(corpus[:2])\n",
    "# Format in (word id, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = news_articles.tags.values.tolist()\n",
    "tags_list = [re.sub(\"\\{|\\}\", \"\", tag) for tag in tags_list]\n",
    "total_tags = []\n",
    "for i in tags_list:\n",
    "    total_tags.extend(i.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tag_count = Counter(total_tags)\n",
    "tag_count.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering tags are supposed to be represent the notion of a \"topic\", looking at the tag distribution is helpful to understand different topic numbers to consider for the model. At a glance of the top 50 tags, it seems like there are anywhere between 5-15 topics, depending on how sparse they are. I will now consider a range of values to create the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c) first, Coherence model for each run of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LDA Model\n",
    "# Build LDA model\n",
    "num_topics = [5,10,12,14,16,18,20,25,30]\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "for i in num_topics:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=i, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    model_list.append(lda_model)\n",
    "    coherencemodel = gensim.models.CoherenceModel(model=lda_model, texts=bigrams_text, dictionary=id2word, coherence='c_v') \n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's figure out the optimal topic value\n",
    "plt.plot(num_topics, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend((\"c_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like 5 is the best, so the first model\n",
    "bst_model = model_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) List of topics and top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = bst_model.show_topics(formatted=False)\n",
    "topics\n",
    "bst_model.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) Top 10 documents related to each topic & topic proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty df\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through model corpus\n",
    "for i, row in enumerate(bst_model[corpus]):\n",
    "    row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "    # Gather document-level information\n",
    "    for k, (num_topic, topic_prop) in enumerate(row):\n",
    "        if k == 0:\n",
    "            results_df = results_df.append(pd.Series([int(num_topic), round(topic_prop,2)]), ignore_index=True)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "addtl_lookup1 = pd.Series(news_articles.article_id.to_list())\n",
    "addtl_lookup2 = pd.Series(news_articles.article_title.to_list())\n",
    "results_df = pd.concat([results_df, addtl_lookup2, addtl_lookup1], axis=1)\n",
    "\n",
    "results_df.columns = ['topic_number', 'contribution_perc','article_title','article_id']\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 10 documents per topic:\n",
    "results_df.sort_values(['topic_number','contribution_perc'],ascending=False).groupby('topic_number').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the different \"top\" articles in the 5 topics, we see that there are very interesting results. There is a clear distinction between articles in different languages. Also, that an article in French and an article in Spanish were in the same topic highlights language similarities between the two. Although the actual content subjects in the traditional sense of the topic were not as separated, there appears to be some clear distinctions too: notably food & the arts, financial news, and then tech news. Conclusion being, while more tuning (and potentially a different model) may have been able to create more granular topics in alignment with the tags above, I believe the following topics do an adequate job separating the articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
