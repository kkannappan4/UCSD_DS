{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2:\n",
    "\n",
    "## Part I:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <li><strong>T2V(WB.Drama,UB.Books.Genre,\"Drama\")</strong>.<br>The entries within WorldBooks' Drama table should be mapped into the USBooks' genre column, specifically with the value \"Drama\".</li>\n",
    "  <br>\n",
    "  <li><strong>T2T(WB.USSales,UB.Sales,-)</strong>.<br>The structure of WorldBooks' US Sales table is the same as the USBooks' Sales table.</li>\n",
    "  <br>\n",
    "  <li><strong>A2A(WB.USSales.total,UB.Sales.total,-)</strong>.<br>The entries within WorldBooks' total sales column in its US Sales table should be mapped directly into the USBooks' Sales total sales column without any transformations applied.</li>\n",
    "  <br>\n",
    "    <li><strong>V2T(WB.Stores.id,UB.Stores.id,[generated(GlobalID),UNION(WB.Stores.id,UB.Stores.id)])</strong>.<br>The id from both WorldBooks' and USBooks' tables are unioned together, where a global ID is generated from the result. The columns are stored in a new table, StoreIdMap.</li>\n",
    "  <br>\n",
    "    <li><strong>A2V([WB.Authors.FirstName,WB.Authors.MiddleName,WB.Authors.LastName],UB.Authors.Name, CONCAT([WB.Authors.FirstName,WB.Authors.MiddleName,WB.Authors.LastName])=UB.Authors.Name)</strong>.<br>The various name entries in WorldBooks' Authors table should be concatenated into the USBooks' Authors name column.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part II:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the attached R script to indicate that I am sampling 300K random rows to be used in my calculations - this is to reduce overall computational complexity. I attached the sample of records actually used in the zip file. \n",
    "\n",
    "I chose to use R where I felt more comfortable writing an insert function that batches the rows efficiently and reduces memory required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import dedupe\n",
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KEY_FIELD = 'visitor_id'\n",
    "SOURCE_TABLE = 'visitors'\n",
    "\n",
    "FIELDS =  [{'field': 'firstname', 'variable name': 'firstname',\n",
    "           'type': 'String','has missing': True},\n",
    "           {'field': 'lastname', 'variable name': 'lastname',\n",
    "           'type': 'String','has missing': True},\n",
    "           {'field': 'uin', 'variable name': 'uin',\n",
    "           'type': 'String','has missing': True},\n",
    "           {'field': 'meeting_loc', 'variable name': 'meeting_loc',\n",
    "           'type': 'String','has missing': True}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deduper = dedupe.Dedupe(FIELDS)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "deduper.classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con = psycopg2.connect(database='',\n",
    "                          host='',\n",
    "                           password='',\n",
    "                          cursor_factory=DictCursor)\n",
    "    print (\"I've connected\")\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "c = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total rows\n",
    "c.execute(\"select count(*) from visitors\")\n",
    "total_unique = c.fetchone()\n",
    "cnt = total_unique['count']\n",
    "print(cnt)\n",
    "\n",
    "# Check distinct firstname and lastname on the same day:\n",
    "c.execute(\"select count(*) from (select distinct firstname, lastname, apptmade::date from visitors where apptmade <> 'None') a\")\n",
    "unique_days = c.fetchone()\n",
    "cnt2 = unique_days['count']\n",
    "print(cnt2)\n",
    "\n",
    "print(\"Differentiating between same day visitor names can yield:\",cnt-cnt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume now, to reduce further computation down the line, that there is an extreme level of unlikelihood that 2 separate people visited the white house on the same day, with the same full name. Already, it is highly unlikely you meet someone with the exact full name that you have - let alone on the same day in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to pre-reduce roles, issue with gathering all attributes and de-duping - yet able to save a few instead of just\n",
    "# complete distinct\n",
    "c.execute(\"select count(*) from (select a.firstname, a.lastname, b.uin, b.meeting_loc \\\n",
    "from \\\n",
    "(select distinct firstname, lastname, apptmade::date \\\n",
    "from visitors \\\n",
    "where apptmade <> 'None') a \\\n",
    "join \\\n",
    "(select distinct firstname, lastname, uin, meeting_loc, apptmade::date \\\n",
    "from visitors where apptmade <> 'None') b \\\n",
    "on a.firstname = b.firstname and a.lastname = b.lastname and a.apptmade = b.apptmade \\\n",
    "union \\\n",
    "select firstname, lastname, uin, meeting_loc \\\n",
    "from visitors \\\n",
    "where apptmade = 'None') c\")\n",
    "row = c.fetchone()\n",
    "count = row['count']\n",
    "# Keeping sample size to 10% given computational constraints\n",
    "sample_size = int(count * .1)\n",
    "print(count)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I elected to remove null values here for scalability purposes, especially when you consider the percentage of incomplete records in the total dataset. I am making this decision without context beyond the task at hand and believing this to be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying for complete records\n",
    "print ('Generating sample of {} records'.format(sample_size))\n",
    "with con.cursor('deduper') as c_deduper:\n",
    "    c_deduper.execute(\"select a.firstname, a.lastname, b.uin, b.meeting_loc \\\n",
    "from \\\n",
    "(select distinct firstname, lastname, apptmade::date \\\n",
    "from visitors \\\n",
    "where apptmade <> 'None') a \\\n",
    "join \\\n",
    "(select distinct firstname, lastname, uin, meeting_loc, apptmade::date \\\n",
    "from visitors where apptmade <> 'None') b \\\n",
    "on a.firstname = b.firstname and a.lastname = b.lastname and a.apptmade = b.apptmade \\\n",
    "union \\\n",
    "select firstname, lastname, uin, meeting_loc \\\n",
    "from visitors \\\n",
    "where apptmade = 'None'\")\n",
    "    temp_d = dict((i, row) for i, row in enumerate(c_deduper))\n",
    "    deduper.sample(temp_d, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Begin Active Learning Portion')\n",
    "dedupe.convenience.consoleLabel(deduper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"Oracle\" has determined that he is satisfied with the current matches presented and is ready to begin the model. In this case, they assumed that common nicknames applied and allowed for liberal misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pre Model Training')\n",
    "deduper.prepare_training(temp_d, sample_size =sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Begin Training')\n",
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Save training file to JSON format: {}'.format('train.json'))\n",
    "with open('train.json', 'w') as training_file:\n",
    "    deduper.writeTraining(training_file)\n",
    "\n",
    "deduper.cleanupTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Creating blocking_map table')\n",
    "c.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS blocking_map\n",
    "    \"\"\")\n",
    "c.execute(\"\"\"\n",
    "    CREATE TABLE blocking_map\n",
    "    (block_key VARCHAR(200), visitor_id INTEGER)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for field in deduper.blocker.index_fields:\n",
    "    print ('Selecting distinct values for \"{}\"'.format(field))\n",
    "    c_index = con.cursor('index')\n",
    "    c_index.execute(\"\"\"\n",
    "        SELECT DISTINCT %s FROM %s\n",
    "        \"\"\" % (field, SOURCE_TABLE))\n",
    "    field_data = (row[field] for row in c_index)\n",
    "    deduper.blocker.index(field_data, field)\n",
    "    c_index.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Generating blocking map')\n",
    "c_block = con.cursor('block')\n",
    "c_block.execute(\"\"\"\n",
    "    SELECT * FROM %s\n",
    "    \"\"\" % SOURCE_TABLE)\n",
    "full_data = ((row[KEY_FIELD], row) for row in c_block)\n",
    "b_data = deduper.blocker(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Inserting blocks into blocking_map')\n",
    "csv_file = tempfile.NamedTemporaryFile(prefix='blocks_', delete=False, mode ='w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "for x in b_data:\n",
    "    csv_writer.writerow(list(x))\n",
    "#csv_writer.writerows(b_data)\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(csv_file.name, 'r')\n",
    "c.copy_expert(\"COPY blocking_map FROM STDIN CSV\", f)\n",
    "f.close()\n",
    "\n",
    "os.remove(csv_file.name)\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Indexing blocks')\n",
    "c.execute(\"\"\"\n",
    "    CREATE INDEX blocking_map_key_idx ON blocking_map (block_key)\n",
    "    \"\"\")\n",
    "c.execute(\"DROP TABLE IF EXISTS plural_key\")\n",
    "c.execute(\"DROP TABLE IF EXISTS plural_block\")\n",
    "c.execute(\"DROP TABLE IF EXISTS covered_blocks\")\n",
    "c.execute(\"DROP TABLE IF EXISTS smaller_coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Calculating plural_key')\n",
    "c.execute(\"\"\"\n",
    "    CREATE TABLE plural_key\n",
    "    (block_key VARCHAR(200),\n",
    "    block_id SERIAL PRIMARY KEY)\n",
    "    \"\"\")\n",
    "c.execute(\"\"\"\n",
    "    INSERT INTO plural_key (block_key)\n",
    "    SELECT block_key FROM blocking_map\n",
    "    GROUP BY block_key HAVING COUNT(*) > 1\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Indexing block_key')\n",
    "c.execute(\"\"\"\n",
    "    CREATE UNIQUE INDEX block_key_idx ON plural_key (block_key)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Calculating plural_block')\n",
    "c.execute(\"\"\"\n",
    "    CREATE TABLE plural_block\n",
    "    AS (SELECT block_id, visitor_id\n",
    "    FROM blocking_map INNER JOIN plural_key\n",
    "    USING (block_key))\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Adding {} index'.format(KEY_FIELD))\n",
    "c.execute(\"\"\"\n",
    "    CREATE INDEX plural_block_visitor_id_idx\n",
    "    ON plural_block (visitor_id)\n",
    "    \"\"\")\n",
    "c.execute(\"\"\"\n",
    "    CREATE UNIQUE INDEX plural_block_block_id_visitor_id_uniq\n",
    "    ON plural_block (block_id, visitor_id)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Creating covered_blocks')\n",
    "c.execute(\"\"\"\n",
    "    CREATE TABLE covered_blocks AS\n",
    "    (SELECT visitor_id,\n",
    "    string_agg(CAST(block_id AS TEXT), ','\n",
    "    ORDER BY block_id) AS sorted_ids\n",
    "    FROM plural_block\n",
    "    GROUP BY 1)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Indexing covered_blocks')\n",
    "c.execute(\"\"\"\n",
    "    CREATE UNIQUE INDEX covered_blocks_visitor_id_idx\n",
    "    ON covered_blocks (visitor_id)\n",
    "    \"\"\")\n",
    "print ('Committing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Creating smaller_coverage')\n",
    "c.execute(\"\"\"\n",
    "    CREATE TABLE smaller_coverage AS\n",
    "    (SELECT visitor_id, block_id,\n",
    "    TRIM(',' FROM split_part(sorted_ids,\n",
    "    CAST(block_id AS TEXT), 1))\n",
    "    AS smaller_ids\n",
    "    FROM plural_block\n",
    "    INNER JOIN covered_blocks\n",
    "    USING (visitor_id))\n",
    "    \"\"\")\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Clustering...')\n",
    "c_cluster = con.cursor('cluster')\n",
    "c_cluster.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM smaller_coverage\n",
    "    INNER JOIN %s\n",
    "    USING (%s)\n",
    "    ORDER BY (block_id)\n",
    "    \"\"\" % (SOURCE_TABLE, KEY_FIELD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def candidates_gen(result_set):\n",
    "    lset = set\n",
    "    block_id = None\n",
    "    records = []\n",
    "    i = 0\n",
    "    for row in result_set:\n",
    "        if row['block_id'] != block_id:\n",
    "            if records:\n",
    "                yield records\n",
    "            \n",
    "            block_id = row['block_id']\n",
    "            records = []\n",
    "            i += 1\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print ('{} blocks'.format(i))\n",
    "\n",
    "        smaller_ids = row['smaller_ids']\n",
    "        if smaller_ids:\n",
    "            smaller_ids = lset(smaller_ids.split(','))\n",
    "        else:\n",
    "            smaller_ids = lset([])\n",
    "        \n",
    "        records.append((row[KEY_FIELD], row, smaller_ids))\n",
    "    \n",
    "    if records:\n",
    "        yield records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered_dupes = deduper.matchBlocks(candidates_gen(c_cluster), threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Creating entity_map table')\n",
    "c.execute(\"DROP TABLE IF EXISTS entity_map\")\n",
    "c.execute(\"\"\"\n",
    "      CREATE TABLE entity_map (\n",
    "      %s INTEGER,\n",
    "      canon_id INTEGER,\n",
    "      cluster_score FLOAT,\n",
    "      PRIMARY KEY(%s)\n",
    "      )\"\"\" % (KEY_FIELD, KEY_FIELD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('Inserting entities into entity_map')\n",
    "for cluster, scores in clustered_dupes:\n",
    "    cluster_id = cluster[0]\n",
    "    for key_field, score in zip(cluster, scores):\n",
    "        c.execute(\"\"\"\n",
    "              INSERT INTO entity_map\n",
    "              (%s, canon_id, cluster_score)\n",
    "              VALUES (%s, %s, %s)\n",
    "              \"\"\" % (KEY_FIELD, key_field, cluster_id, score))\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was able to generate result sets on a sample of data, further computational power may yield the ability to test on the full data-set. The methods applied were both active and semi-supervised learning combined with some assumptions. There may be even more opportunities with more context to reduce the data-set size prior to using dedupe.\n",
    "\n",
    "I have attached the (sample) blocks in the zip file. Improvements for the next iteration include implementing a batch insert function in python similar to the one written in R. Final entity blocks were ran for 2 hours of computation time and were subsequently cut off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
