{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Data-Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from collections import defaultdict,Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Given functions for reading the data\n",
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train and test data:\n",
    "df = getDF('train.json.gz')\n",
    "test_df = getDF('test_Helpful.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial training + validation view\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial test view\n",
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls:\n",
    "print(\"Train:\")\n",
    "print(df.isnull().sum()/df.shape[0])\n",
    "print('\\nTest:')\n",
    "print(test_df.isnull().sum()/df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price feature will be dropped, since substantial amount of training data is not present, does not make sense to bias the results, even if the test set has it more readily available.\n",
    "\n",
    "Considering we have text data, may be valuable to extract a review length column and convert the date column to something meaningful. My preliminary thoughts are that the helpfulness feature could have been retro-actively added, hence it may not have been used early on. My other thought is that seasonality could be at play for shopping, hence some people valuing more comprehensive views around the holidays, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['price'], axis=1,inplace=True)\n",
    "test_df.drop(['price'], axis=1,inplace=True)\n",
    "df['date'] = pd.to_datetime(df['unixReviewTime'],unit='s')\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "test_df['date'] = pd.to_datetime(test_df['unixReviewTime'],unit='s')\n",
    "test_df['year'] = test_df['date'].dt.year\n",
    "test_df['month'] = test_df['date'].dt.month\n",
    "df['len_review'] = df.reviewText.apply(lambda x: len(str(x).split(' ')))\n",
    "test_df['len_review'] = test_df.reviewText.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at category distribution\n",
    "df['categoryID'].value_counts().plot.bar()\n",
    "plt.title('Distribution of Categories Across Training Set')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that category distribution is not even, and there is a dispraportionate distribution of category = 0.\n",
    "\n",
    "For use in the algorithm, category ID must be converted to \"one-hot\" as it is not a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = pd.get_dummies(df['categoryID']).rename(columns=lambda x: 'category_'+str(x))\n",
    "df = pd.concat([df,categories],axis=1)\n",
    "categories_t = pd.get_dummies(test_df['categoryID']).rename(columns=lambda x: 'category_'+str(x))\n",
    "test_df = pd.concat([test_df,categories_t],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rationale behind using sentiment on the summary is that we are still programmed to look at headlines, my guess is the more a comment appears to be attention grabbing (either extremely negative or positive), the more it will be viewed. The more it will be viewed, the more likely it could be classified as helpful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us add some sentiment factor into the model:\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "analyser = SIA()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score.get(\"compound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary compound sentiment:\n",
    "df['summary_sent'] = df.summary.apply(lambda x: sentiment_analyzer_scores(x))\n",
    "test_df['summary_sent'] = test_df.summary.apply(lambda x: sentiment_analyzer_scores(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step, clean target variable and extract information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpful = pd.DataFrame.from_dict(dict(df['helpful'])).T\n",
    "df = pd.concat([df, helpful], axis=1)\n",
    "helpful_t = pd.DataFrame.from_dict(dict(test_df['helpful'])).T\n",
    "test_df = pd.concat([test_df, helpful_t], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training+Validation Data\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test Data\n",
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model value is poor, potentially investigate using more information from the categories:\n",
    "df.iloc[4]['categories'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['rating','year', 'month', 'len_review', 'category_0', \\\n",
    "       'category_1', 'category_2', 'category_3', 'category_4', 'summary_sent', \\\n",
    "       'outOf']\n",
    "fin_test = test_df\n",
    "feature_list.append('nHelpful')\n",
    "fin_train = df[feature_list]\n",
    "fin_train['helpful_rate'] = fin_train['nHelpful']/fin_train['outOf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions:',fin_train.shape)\n",
    "print(fin_train.columns)\n",
    "print(fin_train.isnull().sum()/fin_train.shape[0])\n",
    "fin_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would appear that a good chunk of our data can be represented as a classification problem\n",
    "fin_train['helpful_rate'].value_counts().nlargest(10)/fin_train.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture some summary statistics:\n",
    "fin_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary findings: High variability in target variable, with some outliers. Some decent variation in sentiment analysis. Year and month extraction as features appears to be valuable, for rating feature and seasonality respectively. Rating seems like it will be quite helpful as well. If we look at the ratio between \"Out of\" and \"n Helpful\", we can see that the majority of the results is a 1:1 - meaning a combination of classification and regression should be robust.\n",
    "\n",
    "Now that we have a complete training set, let us visualize some of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_train.hist(bins = 50, figsize=(20,12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cov_mat = fin_train.cov() # to get a heatmap of the covariance matrix\n",
    "cov_plot = sns.heatmap(cov_mat, vmax=1, square = True,cmap=\"Blues\")\n",
    "cov_plot.set_title('Training Set COV Matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat = fin_train.corr() # to get a heatmap of the correlation matrix\n",
    "cor_plot = sns.heatmap(cor_mat, vmax=1, square = True,cmap=\"Blues\")\n",
    "cor_plot.set_title('Training Set Corr. Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that, for the most part, the \"outOf\" feature highly correlates with the target variable. It should be considered the most important feature in the model, with review length also being higher correlrated with the value as well. Based on my analysis, tree based models should be robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV,StratifiedKFold\n",
    "\n",
    "X, y = fin_train.iloc[:,0:11],fin_train.iloc[:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=fin_train.iloc[:,0:11],label=fin_train.iloc[:,11])\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,\n",
    "                'max_depth': 4, 'alpha': 9,'n_estimators':100}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=200,early_stopping_rounds=10,metrics=\"mae\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[20:].drop(['test-mae-std','train-mae-std'],axis=1).plot()\n",
    "plt.xlabel('Boost Round')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Base XGBoost Regression Overfits');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model barely outperforms baseline. To improve it, we will need to apply grid search and use of a validation set across k-fold CV to tune model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fin_results(row):\n",
    "    if row['outOf'] == 0:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = row['xgb_regress']\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mae = []\n",
    "mae_rd = []\n",
    "feature_list = ['rating','year', 'month', 'len_review', 'category_0', \\\n",
    "       'category_1', 'category_2', 'category_3', 'category_4', 'summary_sent', \\\n",
    "       'outOf']\n",
    "iteration = 0\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    print('\\n{} of kfold {}'.format(iteration,kf.n_splits))\n",
    "    xtr,xvl = X.loc[train_index],X.loc[test_index]\n",
    "    ytr,yvl = y[train_index],y[test_index]\n",
    "    CV_params = dict(learning_rate=[0.01, 0.1])\n",
    "    params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,\n",
    "                'max_depth': 4, 'alpha': 9,'n_estimators':100}\n",
    "    xgb_reg = GridSearchCV(xgb.XGBRegressor(**params),CV_params,cv = 10, scoring = 'mean_absolute_error',n_jobs = -1)\n",
    "    xgb_reg.fit(xtr,ytr)\n",
    "    print(\"Best parameters:\")\n",
    "    print(xgb_reg.best_params_)\n",
    "    \n",
    "    # Predicting on the validation set:\n",
    "    xvl = xvl.reset_index(drop = True)\n",
    "    model_perf = pd.DataFrame({'outOf':xvl['outOf'], 'xgb_regress':xgb_reg.predict(xvl)})\n",
    "    model_perf['pred'] = model_perf.apply(fin_results,axis = 1)\n",
    "\n",
    "    \n",
    "    print(\"MAE w/o rounding: %0.3f\" % mean_absolute_error(list(yvl), list(model_perf.pred)))\n",
    "    mae.append(mean_absolute_error(list(yvl), list(model_perf.pred)))\n",
    "    print(\"MAE w/ rounding: %0.3f\" % mean_absolute_error(list(yvl), list(np.round(model_perf.pred))))\n",
    "    mae_rd.append(mean_absolute_error(list(yvl), list(np.round(model_perf.pred))))\n",
    "    iteration += 1\n",
    "     \n",
    "print('Without rounding, \"mean\" mae: %0.3f' % np.mean(mae))\n",
    "print('With rounding, \"mean\" mae: %0.3f' % np.mean(mae_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 60, 30\n",
    "xgb.plot_tree(xgb_reg.best_estimator_,num_trees=2)\n",
    "plt.rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model\n",
    "features = xvl.columns\n",
    "importances = xgb_reg.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to model the feature directly is not the right approach as there is too much variability present and heavily relies on outOf (see example tree): the base model was better off. Next, we want to implement some sort of iterated classifier based on the ratio. Should be a linear combination of 'outOf' and the ratio.\n",
    "\n",
    "Additionally, these libraries were not properly allowing for iterated K fold and grid search based on said folds, creating new validation sets. We implement an iterated random approach to test on new validation sets each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_train_split(dataframe, features, target, random_state):\n",
    "    X_train = pd.DataFrame(dataframe, columns=features)\n",
    "    y_train = pd.DataFrame(dataframe[target])\n",
    "    X_train_n, X_valid, y_train_n, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=i)\n",
    "    return X_train_n, y_train_n, X_valid, y_valid\n",
    "\n",
    "def fin_results2(row):\n",
    "    if row['outOf'] == 0:\n",
    "        val = 0\n",
    "    elif row['outOf'] == 1:\n",
    "        val = row['outOf']*row['xgb_class']\n",
    "    else:\n",
    "        val = row['outOf']*row['xgb_reg']\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict, StratifiedKFold \n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mae = []\n",
    "mae_rd = []\n",
    "features = ['rating', 'year', 'month', 'len_review', 'category_0', 'category_1',\\\n",
    "       'category_2', 'category_3', 'category_4', 'summary_sent', 'outOf']\n",
    "iteration = 0\n",
    "for i in range(5):\n",
    "    print('\\n{} of kfold {}'.format(iteration,5))\n",
    "    X_train_n, y_train_n, X_valid, y_valid = n_train_split(fin_train, features, ['helpful_rate', 'nHelpful'], i)\n",
    "    \n",
    "    # Leverage binary representation for classifier when ratio = 1\n",
    "    X_train_cl = X_train_n[X_train_n['outOf'] == 1]\n",
    "    y_train_cl = np.array(y_train_n['helpful_rate'][X_train_n['outOf'] == 1])\n",
    "    kf_cl = StratifiedKFold(y_train_cl, n_folds=5, shuffle=True, random_state=0)\n",
    "    grid_search = dict(learning_rate=[0.01,0.05,0.1],loss=['deviance','exponential'])\n",
    "    params = {'n_estimators': 100, 'max_depth': 4}\n",
    "    gbclf = GridSearchCV(ensemble.GradientBoostingClassifier(**params), grid_search, cv=kf_cl, scoring='mean_absolute_error',n_jobs=-1)\n",
    "    gbclf.fit(X_train_cl, y_train_cl)\n",
    "    print(\"Best classification parameters:\")\n",
    "    print(gbclf.best_params_)\n",
    "    \n",
    "    # Utilize regression otherwise\n",
    "    X_train_reg = X_train_n[(X_train_n['outOf'] != 0) & (X_train_n['outOf'] != 1)]\n",
    "    y_train_reg = np.array(y_train_n['helpful_rate'][(X_train_n['outOf'] != 0) & (X_train_n['outOf'] != 1)])\n",
    "    kf_reg = StratifiedKFold(y_train_reg, n_folds=5, shuffle=True, random_state=0)\n",
    "    grid_search = dict(learning_rate=[0.01,0.05,0.1], loss=['ls', 'lad'])\n",
    "    params = {'n_estimators': 100, 'max_depth': 4}\n",
    "    gbreg = GridSearchCV(ensemble.GradientBoostingRegressor(**params), grid_search, cv=kf_reg, scoring='mean_absolute_error', n_jobs=-1)\n",
    "    gbreg.fit(X_train_reg, y_train_reg)\n",
    "    print(\"Best regression parameters:\")\n",
    "    print(gbreg.best_params_)\n",
    "    \n",
    "    # Predicting on the validation set:\n",
    "    X_valid.reset_index(drop=True,inplace=True)\n",
    "    model_perf = pd.DataFrame({'outOf':X_valid['outOf'], 'xgb_class':gbclf.predict(X_valid), 'xgb_reg':gbreg.predict(X_valid)})\n",
    "    model_perf['pred'] = model_perf.apply(fin_results2,axis = 1)\n",
    "    \n",
    "    print(\"MAE w/o rounding: %0.3f\" % mean_absolute_error(list(y_valid['nHelpful']), list(model_perf.pred)))\n",
    "    mae.append(mean_absolute_error(list(y_valid['nHelpful']), list(model_perf.pred)))\n",
    "    print(\"MAE w/ rounding: %0.3f\" % mean_absolute_error(list(y_valid['nHelpful']), list(np.round(model_perf.pred))))\n",
    "    mae_rd.append(mean_absolute_error(list(y_valid['nHelpful']), list(np.round(model_perf.pred))))\n",
    "    iteration += 1\n",
    "\n",
    "print('\\nFinal training results:')\n",
    "print('Without rounding, \"mean\" mae: %0.3f' % np.mean(mae))\n",
    "print('With rounding, \"mean\" mae: %0.3f' % np.mean(mae_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-Classifier\n",
    "features = X_train_cl.columns\n",
    "importances = gbclf.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-Regressor\n",
    "features = X_train_reg.columns\n",
    "importances = gbreg.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Coefficients')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Feature');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mae = []\n",
    "mae_rd = []\n",
    "features = ['rating', 'year', 'month', 'len_review', 'category_0', 'category_1',\\\n",
    "       'category_2', 'category_3', 'category_4', 'summary_sent', 'outOf']\n",
    "iteration = 0\n",
    "for i in range(5):\n",
    "    print('\\n{} of kfold {}'.format(iteration,5))\n",
    "    X_train_n, y_train_n, X_valid, y_valid = n_train_split(fin_train, features, ['helpful_rate', 'nHelpful'], i)\n",
    "    \n",
    "    # Leverage binary representation for classifier when ratio = 1, try logistic regression\n",
    "    X_train_cl = X_train_n[X_train_n['outOf'] == 1]\n",
    "    y_train_cl = np.array(y_train_n['helpful_rate'][X_train_n['outOf'] == 1])\n",
    "    kf_cl = StratifiedKFold(y_train_cl, n_folds=5, shuffle=True, random_state=0)\n",
    "    parameters = { 'penalty': ['l1','l2'], \n",
    "              'C':[0.1, 0.5, 1, 2, 3, 4, 5, 10]}\n",
    "    logreg = LogisticRegression()\n",
    "    logclf = GridSearchCV(logreg, parameters, cv=kf_cl, scoring='mean_absolute_error',n_jobs=-1)\n",
    "    logclf.fit(X_train_cl, y_train_cl)\n",
    "    print(\"Best classification parameters:\")\n",
    "    print(logclf.best_params_)\n",
    "    \n",
    "    # Utilize regression otherwise\n",
    "    X_train_reg = X_train_n[(X_train_n['outOf'] != 0) & (X_train_n['outOf'] != 1)]\n",
    "    y_train_reg = np.array(y_train_n['helpful_rate'][(X_train_n['outOf'] != 0) & (X_train_n['outOf'] != 1)])\n",
    "    kf_reg = StratifiedKFold(y_train_reg, n_folds=5, shuffle=True, random_state=0)\n",
    "    grid_search = dict(learning_rate=[0.01,0.05,0.1], loss=['ls', 'lad'])\n",
    "    params = {'n_estimators': 100, 'max_depth': 4}\n",
    "    gbreg = GridSearchCV(ensemble.GradientBoostingRegressor(**params), grid_search, cv=kf_reg, scoring='mean_absolute_error', n_jobs=-1)\n",
    "    gbreg.fit(X_train_reg, y_train_reg)\n",
    "    print(\"Best regression parameters:\")\n",
    "    print(gbreg.best_params_)\n",
    "    \n",
    "    # Predicting on the validation set:\n",
    "    X_valid.reset_index(drop=True,inplace=True)\n",
    "    model_perf = pd.DataFrame({'outOf':X_valid['outOf'], 'xgb_class':logclf.predict(X_valid), 'xgb_reg':gbreg.predict(X_valid)})\n",
    "    model_perf['pred'] = model_perf.apply(fin_results2,axis = 1)\n",
    "    \n",
    "    print(\"MAE w/o rounding: %0.3f\" % mean_absolute_error(list(y_valid['nHelpful']), list(model_perf.pred)))\n",
    "    mae.append(mean_absolute_error(list(y_valid['nHelpful']), list(model_perf.pred)))\n",
    "    print(\"MAE w/ rounding: %0.3f\" % mean_absolute_error(list(y_valid['nHelpful']), list(np.round(model_perf.pred))))\n",
    "    mae_rd.append(mean_absolute_error(list(y_valid['nHelpful']), list(np.round(model_perf.pred))))\n",
    "    iteration += 1\n",
    "\n",
    "print('\\nFinal training results:')\n",
    "print('Without rounding, \"mean\" mae: %0.3f' % np.mean(mae))\n",
    "print('With rounding, \"mean\" mae: %0.3f' % np.mean(mae_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us gather the parameters to properly export:\n",
    "\n",
    "# GB Regressor: across both CVs, we know best tuning parameter: {'learning_rate': 0.1, 'loss': 'lad'}\n",
    "print('Full GB regress model parameters:\\n\\n',gbreg.best_estimator_,'\\n')\n",
    "\n",
    "# Logistic Regression, using best GB Regressor, tune parameters to: {'C': 0.1, 'penalty': 'l1'}\n",
    "print('Full Logistic model parameters:\\n\\n',logclf.best_estimator_,'\\n')\n",
    "\n",
    "# GB Classifier, tune parameters to EITHER: {'learning_rate': 0.05, 'loss': 'exponential'} OR {'learning_rate': 0.05, 'loss': 'deviance'}\n",
    "print('Full GB classifier model parameters:\\n\\n',gbclf.best_estimator_,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 2 submissions (log reg + xgb regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "X_train_fin = fin_train[features]\n",
    "y_train_fin=pd.DataFrame(fin_train['helpful_rate'])\n",
    "\n",
    "# Perform predictions on the test set, using full training data: first logistic regression\n",
    "X_train_cl = X_train_fin[X_train_fin['outOf'] == 1]\n",
    "y_train_cl = np.array(y_train_fin['helpful_rate'][X_train_fin['outOf'] == 1])\n",
    "logclf = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False) \n",
    "\n",
    "logclf.fit(X_train_cl, y_train_cl)\n",
    "    \n",
    "# Utilize regression otherwise:\n",
    "X_train_reg = X_train_fin[(X_train_fin['outOf'] != 0) & (X_train_fin['outOf'] != 1)]\n",
    "y_train_reg = np.array(y_train_fin['helpful_rate'][(X_train_fin['outOf'] != 0) & (X_train_fin['outOf'] != 1)])\n",
    "gbreg = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='lad', max_depth=4, max_features=None,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=100, presort='auto', random_state=None,\n",
    "             subsample=1.0, verbose=0, warm_start=False) \n",
    "gbreg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "# Predicting on the test set:\n",
    "test_set = pd.DataFrame(fin_test,columns = features)\n",
    "final_model1 = pd.DataFrame({'outOf':test_set['outOf'], 'xgb_class':logclf.predict(test_set), 'xgb_reg':gbreg.predict(test_set)})\n",
    "final_model1['pred'] = final_model1.apply(fin_results2,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model1.rename(columns={'xgb_class':'log_reg','pred': 'prediction'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_und1 = pd.concat([fin_test[['reviewerID','itemID','outOf']],final_model1[['prediction']]],axis=1)\n",
    "export_und2 = pd.concat([fin_test[['reviewerID','itemID','outOf']],final_model1[['prediction']]],axis=1)\n",
    "export_und2['prediction'] = np.round(final_model1['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_export(row):\n",
    "    val = row['reviewerID'] + '-'+ row['itemID'] + '-' + str(row['outOf'])\n",
    "    \n",
    "    return val\n",
    "\n",
    "export_und1['userID-itemID-outOf'] = export_und1.apply(concat_export,axis = 1)\n",
    "export_und2['userID-itemID-outOf'] = export_und2.apply(concat_export,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_und1[['userID-itemID-outOf','prediction']].to_csv('log_reg_nround.csv',index=False)\n",
    "export_und2[['userID-itemID-outOf','prediction']].to_csv('log_reg_round.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second 2 submissions (gb clf + gb reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's try the gradient booster, potentially logistic regression has overfit:\n",
    "warnings.filterwarnings('ignore')\n",
    "X_train_fin = fin_train[features]\n",
    "y_train_fin=pd.DataFrame(fin_train['helpful_rate'])\n",
    "\n",
    "# Perform predictions on the test set, using full training data: first logistic regression (change exponential or deviance)\n",
    "X_train_cl = X_train_fin[X_train_fin['outOf'] == 1]\n",
    "y_train_cl = np.array(y_train_fin['helpful_rate'][X_train_fin['outOf'] == 1])\n",
    "gbclf = ensemble.GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='exponential', max_depth=4,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_split=None, min_samples_leaf=1,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=100, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)\n",
    "\n",
    "gbclf.fit(X_train_cl, y_train_cl)\n",
    "    \n",
    "# Utilize regression otherwise:\n",
    "X_train_reg = X_train_fin[(X_train_fin['outOf'] != 0) & (X_train_fin['outOf'] != 1)]\n",
    "y_train_reg = np.array(y_train_fin['helpful_rate'][(X_train_fin['outOf'] != 0) & (X_train_fin['outOf'] != 1)])\n",
    "gbreg = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='lad', max_depth=4, max_features=None,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=100, presort='auto', random_state=None,\n",
    "             subsample=1.0, verbose=0, warm_start=False) \n",
    "gbreg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "# Predicting on the test set:\n",
    "test_set = pd.DataFrame(fin_test,columns = features)\n",
    "final_model2 = pd.DataFrame({'outOf':test_set['outOf'], 'xgb_class':gbclf.predict(test_set), 'xgb_reg':gbreg.predict(test_set)})\n",
    "final_model2['pred'] = final_model2.apply(fin_results2,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model2.rename(columns={'pred': 'prediction'}, inplace=True)\n",
    "export_und3 = pd.concat([fin_test[['reviewerID','itemID','outOf']],final_model2[['prediction']]],axis=1)\n",
    "export_und4 = pd.concat([fin_test[['reviewerID','itemID','outOf']],final_model2[['prediction']]],axis=1)\n",
    "export_und4['prediction'] = np.round(final_model2['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_und3['userID-itemID-outOf'] = export_und3.apply(concat_export,axis = 1)\n",
    "export_und4['userID-itemID-outOf'] = export_und4.apply(concat_export,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_und3[['userID-itemID-outOf','prediction']].to_csv('05exp_xgb_clf_nround.csv',index=False)\n",
    "export_und4[['userID-itemID-outOf','prediction']].to_csv('05exp_xgb_clf_round.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
