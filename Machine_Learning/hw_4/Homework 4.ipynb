{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "Problem 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import data-set\n",
    "from nltk.corpus import brown\n",
    "brown_words = brown.words()\n",
    "print(len(brown_words), 'total words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process\n",
    "import string\n",
    "brown_words = [''.join(c for c in s if c not in string.punctuation) for s in brown_words]\n",
    "brown_words = [x.lower() for x in brown_words if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "brown_words2 = [word for word in brown_words if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "brown_counts = Counter(brown_words2)\n",
    "sorted_brown = sorted(brown_counts.items(), key=lambda kv: kv[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [x[0] for x in sorted_brown[:5000]]\n",
    "context_w = [x[0] for x in sorted_brown[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_matrix = np.zeros((5000, 1000))\n",
    "\n",
    "# Follow directions exactly, very inefficient -- possibility to parallelize\n",
    "for w in vocabulary:\n",
    "    v_index = [i for i,val in enumerate(vocabulary) if val==w]\n",
    "    store_words = []\n",
    "    indices = [i for i,val in enumerate(brown_words2) if val==w]\n",
    "    for i in indices:\n",
    "        if i == 0:\n",
    "            store = brown_words2[i+1:i+3]\n",
    "        elif i == 1:\n",
    "            store = brown_words2[i-1:i] + brown_words2[i+1:i+3]\n",
    "        else:\n",
    "            store = brown_words2[i-2:i] + brown_words2[i+1:i+3]\n",
    "        store_words.append(store)\n",
    "    for store in store_words:\n",
    "        for word in store:\n",
    "            if word in context_w:\n",
    "                c_index = [i for i,val in enumerate(context_w) if val==word]\n",
    "                context_matrix[v_index, c_index] += 1\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_context_words = context_matrix/context_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = context_matrix.sum(axis=0)\n",
    "total_context = c.sum()\n",
    "pr_context = c/total_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_cw_df = pd.DataFrame(pr_context_words, index=vocabulary, columns=context_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_info = np.log(pr_context_words/pr_context)\n",
    "m_info[m_info < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5:\n",
    "\n",
    "Let us use PCA to reduce the dimensionality to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "df_phi_w = pd.DataFrame(m_info, index=vocabulary, columns=context_w)\n",
    "pca = PCA(n_components=100, random_state=10)\n",
    "pca_m_info = pca.fit_transform(m_info)\n",
    "pca_m_info = pd.DataFrame(pca_m_info, index=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phi_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "print('For PCA(100), ', pca.explained_variance_ratio_.sum()* 100.0, '% of the variance is explained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import K Means Package\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set k = 100\n",
    "km100 = KMeans(n_init=6, n_clusters=100, max_iter= 1000, init='k-means++', random_state=0)\n",
    "km100.fit(pca_m_info)\n",
    "# Get cluster assignment labels\n",
    "labels = km100.labels_\n",
    "# Format results as a DataFrame\n",
    "results = pd.DataFrame([pca_m_info.index,labels]).T\n",
    "results.columns = ['class', 'cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cluster for economics:\\n',results.groupby('cluster')['class'].apply(list)[4],'\\n')\n",
    "print('Cluster for state affairs:\\n',results.groupby('cluster')['class'].apply(list)[8],'\\n')\n",
    "print('Cluster for state affairs (2):\\n',results.groupby('cluster')['class'].apply(list)[12],'\\n')\n",
    "print('Cluster for quantities:\\n',results.groupby('cluster')['class'].apply(list)[6],'\\n')\n",
    "print('Cluster for names or pronouns:\\n',results.groupby('cluster')['class'].apply(list)[13],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K-Means ++, it would appear that the clusters, while some of them appear to be random, for the most part have some logical sense. Following the directions of the problem, the clusters selected appear to be the most salient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import KNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Function to achieve nearest neighbor\n",
    "def knn_info(brown_words2):\n",
    "    for word in brown_words2:\n",
    "        subset = pca_m_info.drop(word)\n",
    "        neigh = NearestNeighbors(n_neighbors=1, algorithm='brute', metric='cosine')\n",
    "        neigh.fit(subset)\n",
    "        nn_loc = neigh.kneighbors(pca_m_info[pca_m_info.index == word])[1]\n",
    "        print('For', word, ', the nearest neighbor is = ', subset.index[nn_loc][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['communism','autumn','cigarette','pulmonary','mankind','africa','chicago',\\\n",
    "         'revolution','september','chemical','detergent','dictionary','storm','worship',\\\n",
    "         'employees','million','wife','husband','education','world','christ','would','cattle', \\\n",
    "         'thousand','new']\n",
    "knn_info(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my interpretation, KNN does a very good job teasing out insights from the selection of words chosen. There are only a few exceptions that require you to think more critically: cigarette + bullet and storm + saturday. Other than that, the neighbors are pretty spot on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
